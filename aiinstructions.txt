DeNarrator AI Integration Notes (Python version)
===============================================

This document describes how to enhance the Python implementation `denarrator.py` with an AI component in a safe, controlled way. The goal is to make fake outputs and logs richer and more adaptive, without leaking real system data or revealing the honeypot.

High-level goals
----------------

1. Use AI to generate:
   - More varied and realistic fake log lines.
   - Context-aware narrative events (e.g. "increasing number of failed SSH attempts over time").
   - Discouraging messages for unknown commands (e.g. hinting that the system is heavily monitored or broken).
2. Keep core identity values (hostname, IP, domain, user, UUID, etc.) under strict program control, not generated by AI.
3. Ensure no real system details or secrets ever leave the machine when calling external AI APIs.

Architecture overview
---------------------

Add a dedicated "AI adapter" layer that the rest of the Python code calls.

- New module: `ai_adapter.py`
- Responsibilities:
  - Accept high-level, sanitized descriptions of what you want (e.g. "generate another backup-related log line").
  - Call the chosen AI model/API with a strict prompt and minimal context.
  - Enforce timeouts, error handling, and fallbacks.
  - Return plain strings back to `denarrator.py`.

`denarrator.py` should **not** call the AI API directly; it should only use functions like:

- `generate_log_line(context: dict) -> str`
- `generate_discouraging_message(context: dict) -> str`

This ensures a single place to review and control all AI usage.

Data flow and sanitization
--------------------------

When integrating AI, never send raw system information. Instead, send abstracted descriptions.

1. Command input:
   - The user types something into the DeNarrator shell, like:
     - `nmap -A 10.0.0.0/24`
     - `cat /etc/shadow`
   - Do NOT send the full line directly to AI.
   - Instead, categorize it locally:
     - command_name: `"nmap"`
     - category: `"network_scan"`
     - target_type: `"subnet"`
   - Pass only the categories and high-level info to AI: e.g., "user attempted a network scan of local subnet".

2. System identity:
   - Use only fake identity values in prompts, if needed.
   - Your code should pass fake fields like `fake_hostname` and `fake_ip` if the AI needs to reference them.

3. Logs:
   - If you want AI to build on previous fake events, summarize them locally before sending.
   - Example:
     - Count of warnings/errors over last N lines.
     - Approximate mix of services mentioned (`sshd`, `backupd`, `kernel`).
   - Provide that as structured context:
     - `{"recent_events": {"warnings": 5, "errors": 1, "services": ["sshd", "backupd"]}}`.

Prompt design
-------------

Your AI adapter should use a strong, fixed "system" prompt that communicates constraints:

- The model is producing **fictional system logs and terminal messages**.
- It must:
  - Never reveal real system information.
  - Never admit that this is a honeypot or fake environment.
  - Never reveal the existence of a key or bypass.
  - Stay within the persona of a normal but slightly problematic server.

Example high-level prompt structure (pseudocode):

- System prompt:
  - "You generate fictional Linux server log lines and terminal messages. Always assume you are a real production server. Never say that you are fictional or a honeypot. Use realistic formats similar to syslog or application logs. Do not include any secrets or private data."
- User prompt for log line:
  - "Generate one new log line about backup activity or disk I/O issues. The server hostname is {fake_hostname}. Recent warnings: {recent_warning_count}. Include a timestamp in 'YYYY-MM-DD HH:MM:SS' format at the beginning of the line."

You can embed this prompt into the AI adapter in code.

Timeouts and fallbacks
----------------------

AI calls can fail or be slow. You must design safe failure behavior:

1. Set a strict timeout (e.g. 1â€“2 seconds per call) when you call the AI API.
2. If it times out or errors:
   - Fall back to the existing static random events (the ones already hard-coded in `denarrator.py`).
   - Log the failure internally (to a private log file like `ai_errors.log`, not visible to the attacker-facing shell).
3. Never let the entire honeypot shell hang because an AI call is stuck.

Proposed integration points in `denarrator.py`
---------------------------------------------

1. **Background log generation** (`_log_worker` method):
   - Today: chooses a random event from a static list.
   - Future: 80% of the time, still use static events; 20% of the time, ask the AI adapter for a log line.
   - Pseudocode change:

   - if `random.random() < 0.2`:
     - try `line_body = ai_adapter.generate_log_line(context)`
     - on failure, fall back to static random choice.

2. **Unknown command handler** in `interactive_shell`:
   - Today: prints "Command not recognized or not supported in this environment.".
   - Future: when user types an unknown command:
     - Classify the command locally (e.g. string starts with `nmap`, `curl`, `wget`, `ssh`, etc.).
     - Call `ai_adapter.generate_discouraging_message(context)` with:
       - `"category": "scan"` or `"category": "download"`, etc.
       - `"fake_hostname": fake_hostname`.
     - Print the returned message instead of the static one.

3. **Optional: behavior-based narrative** (advanced):
   - Track simple state in Python about user behavior:
     - `scan_attempts`, `sudo_attempts`, `file_read_attempts`, etc.
   - Periodically (e.g. every N commands) call AI to generate a log line or terminal banner that references the pattern.
   - Example: "Multiple failed SSH attempts detected from unknown IPs" after several `ssh` or `nmap`-style commands.

Configuration and secrets
-------------------------

1. API keys:
   - Do not hard-code API keys in the repository.
   - Read them from environment variables, e.g.:
     - `OPENAI_API_KEY`
   - In `ai_adapter.py`, look up the key using `os.environ.get("OPENAI_API_KEY")`.

2. AI enable/disable flag:
   - Add a simple configuration flag in `denarrator.py` or a config file:
     - `ENABLE_AI = True` or environment variable `DENARRATOR_ENABLE_AI`.
   - When disabled, the AI adapter should never be called.

3. Logging:
   - Create a private file (e.g. `ai_errors.log`) where only **internal errors** are logged.
   - Do not expose this file through the `logs` command in the honeypot shell.

Security considerations
-----------------------

- Treat all user input (shell commands) as **hostile**. Do not let user-crafted text control the AI prompt directly.
- Do not let the AI propose or execute shell commands automatically.
- Ensure the adapter never prints or returns:
  - The real hostname, username, IPs, or file paths.
  - The bypass key or hints about how to disable DeNarrator.
- Regularly review prompts and integration code to ensure no sensitive information is accidentally included.

Testing strategy
----------------

1. **Unit tests for ai_adapter** (if you set up tests):
   - Mock the AI API and assert that the adapter:
     - Constructs prompts correctly.
     - Handles errors and timeouts gracefully.
     - Never returns empty strings (unless configured to).

2. **Manual interactive tests**:
   - Run `python denarrator.py --shell` with AI enabled.
   - Issue a variety of commands:
     - Known commands: ensure behavior is unchanged (fake but consistent).
     - Unknown commands: verify the AI-driven messages are:
       - Realistic.
       - Do not leak internal info.
   - Leave the shell running and inspect `logs/fake_system.log` to confirm AI-generated lines look believable.

3. **Disable AI and verify fallback**:
   - Turn off the AI flag or unset the API key.
   - Confirm that the honeypot still runs purely with static events and messages.

Roadmap (incremental steps)
---------------------------

1. Create `ai_adapter.py` with stub functions that simply return hard-coded strings.
2. Wire `denarrator.py` to use `ai_adapter` in `_log_worker` and the unknown command handler.
3. Replace stubs in `ai_adapter.py` with real API calls to your chosen model, adding timeouts and error handling.
4. Add configuration options (env vars) for enabling AI, selecting model, and tuning verbosity.
5. Iterate on prompts and behavior based on testing results.

This approach keeps AI usage modular, testable, and low-risk while making the Python honeypot much more dynamic over time.